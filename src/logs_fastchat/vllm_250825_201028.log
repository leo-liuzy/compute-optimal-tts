2025-08-25 20:10:39 | ERROR | stderr | Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
2025-08-25 20:10:39 | ERROR | stderr | 
2025-08-25 20:10:39 | ERROR | stderr | Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.63it/s]
2025-08-25 20:10:39 | ERROR | stderr | 
2025-08-25 20:10:40 | ERROR | stderr | Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.89it/s]
2025-08-25 20:10:40 | ERROR | stderr | 
2025-08-25 20:10:41 | ERROR | stderr | Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.48it/s]
2025-08-25 20:10:41 | ERROR | stderr | 
2025-08-25 20:10:41 | ERROR | stderr | Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]
2025-08-25 20:10:41 | ERROR | stderr | 
2025-08-25 20:10:41 | ERROR | stderr | Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]
2025-08-25 20:10:41 | ERROR | stderr | 
2025-08-25 20:10:41 | ERROR | stderr | 
2025-08-25 20:10:43 | ERROR | stderr | [rank0]: Traceback (most recent call last):
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/runpy.py", line 196, in _run_module_as_main
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     return _run_code(code, main_globals, None,
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/runpy.py", line 86, in _run_code
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     exec(code, run_globals)
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/datastor1/zliu/compute-optimal-tts/src/reason/llm_service/workers/vllm_worker.py", line 270, in <module>
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     engine = AsyncLLMEngine.from_engine_args(engine_args)
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 642, in from_engine_args
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     engine = cls(
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 592, in __init__
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     self.engine = self._engine_class(*args, **kwargs)
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 265, in __init__
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     super().__init__(*args, **kwargs)
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 274, in __init__
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     self._initialize_kv_caches()
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 427, in _initialize_kv_caches
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 119, in initialize_cache
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     self.collective_rpc("initialize_cache",
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 49, in collective_rpc
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/utils.py", line 2208, in run_method
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     return func(*args, **kwargs)
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/worker/worker.py", line 308, in initialize_cache
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     self._init_cache_engine()
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/worker/worker.py", line 313, in _init_cache_engine
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     self.cache_engine = [
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/worker/worker.py", line 314, in <listcomp>
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     CacheEngine(self.cache_config, self.model_config,
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 62, in __init__
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     self.gpu_cache = self._allocate_kv_cache(
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:   File "/u/zliu/datastor1/miniconda3/envs/tts/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 81, in _allocate_kv_cache
2025-08-25 20:10:43 | ERROR | stderr | [rank0]:     torch.zeros(kv_cache_shape,
2025-08-25 20:10:43 | ERROR | stderr | [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 830.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 756.50 MiB is free. Process 418149 has 13.67 GiB memory in use. Including non-PyTorch memory, this process has 29.93 GiB memory in use. Of the allocated memory 29.56 GiB is allocated by PyTorch, and 51.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
